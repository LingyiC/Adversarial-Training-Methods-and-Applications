# Milestone-Code-Group-4
Adversarial training (AT) and its variations can be used as means of regularization by adding small perturbations on the training data for supervised and semi-supervised learning. In the domain of NLP, these methods can only serve as regularization methods and are not able to represent real adversarial examples. Given the fact that informal words and adversarial examples in image, we hypothesis that informal words may be related to adversarial examples given its nature of adding or changing some of the character from formal sentences without the change of meaning. Besides, we plan to study whether models trained with AT methods can perform better on language understanding tasks on informal inputs.

This milestone report explains the method and architect we proposed and provides the preliminary result on our baseline methods and initial experiment on the effect of different regularization methods including AT and virtual adversarial training (VAT).

## Experiment 1: Adversarial Text Classification
 
 ## Experiment 2: Insincere Questions Classification
 
 
