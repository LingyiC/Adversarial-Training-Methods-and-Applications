{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiNjlx7VX1VU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1554724102751,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "ykCa4roWIo0y",
    "outputId": "5eb1bfdf-b1d3-402c-da68-ab4f05ee9fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# You can make a copy, then change and run the code\n",
    "# path need to be changed\n",
    "# ~2 hours to train \n",
    "import sys \n",
    "sys.path.append('/content/gdrive/My Drive/final_pj')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8_wVFxJI-Ej"
   },
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUASUotGLjVH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from tensorflow.keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omyNY_1Ylwyd"
   },
   "outputs": [],
   "source": [
    "def is_number(text):\n",
    "  try:\n",
    "    if text[-1] == 's':\n",
    "      text = text[:-1]\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  try:\n",
    "    if text[-1] == 'h' and text[-2] == 't':\n",
    "      text = text[:-2]\n",
    "  except:\n",
    "    pass\n",
    "    \n",
    "  for digits in text:\n",
    "    if digits not in ['0','1','2','3','4','5','6','7','8','9']:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eVHg1dLLop_"
   },
   "outputs": [],
   "source": [
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"/content/gdrive/My Drive/final_pj/train.csv\")\n",
    "    test_df = pd.read_csv(\"/content/gdrive/My Drive/final_pj/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAawQKICkZm1"
   },
   "outputs": [],
   "source": [
    "def load_and_prec2(stopwords):\n",
    "    train_df = pd.read_csv(\"/content/gdrive/My Drive/final_pj/train.csv\")\n",
    "    test_df = pd.read_csv(\"/content/gdrive/My Drive/final_pj/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X_raw = train_df[\"question_text\"].fillna(\"\").values\n",
    "    train_X_raw = [preprocessing.text.text_to_word_sequence(text, lower=False) for text in train_X_raw]\n",
    "    train_X = []\n",
    "    for line in train_X_raw:\n",
    "      tmp = []\n",
    "      for item in line:\n",
    "        if item in stopwords:\n",
    "          continue\n",
    "        if is_number(item):\n",
    "          tmp.append('NUM')\n",
    "        else:\n",
    "          tmp.append(item)\n",
    "      train_X.append(' '.join(tmp))\n",
    "    del train_X_raw\n",
    "    \n",
    "    test_X_raw = test_df[\"question_text\"].fillna(\"\").values\n",
    "    test_X_raw = [preprocessing.text.text_to_word_sequence(text, lower=False) for text in test_X_raw]\n",
    "    test_X = []\n",
    "    for line in test_X_raw:\n",
    "      tmp = []\n",
    "      for item in line:\n",
    "        if item in stopwords:\n",
    "          continue\n",
    "        if is_number(item):\n",
    "          tmp.append('NUM')\n",
    "        else:\n",
    "          tmp.append(item)\n",
    "      test_X.append(' '.join(tmp))\n",
    "    del test_X_raw\n",
    "  \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen, value=0)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen, value=0)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-umDjfmJNuC"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '/content/gdrive/My Drive/final_pj/embeddings/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '/content/gdrive/My Drive/Final Project/4995 DL/basic classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '/content/gdrive/My Drive/Final Project/4995 DL/basic classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUO5xjVak1YD"
   },
   "outputs": [],
   "source": [
    "def load_fasttext2(word_index): \n",
    "  WIKI_NEWS = '/content/gdrive/My Drive/final_pj/embeddings/wiki-news-300d-1M.vec'\n",
    "  wikinews_model = gensim.models.KeyedVectors.load_word2vec_format(WIKI_NEWS, binary=False)\n",
    "  \n",
    "  nb_words = min(max_features, len(word_index))\n",
    "  embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "  for words, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "      continue\n",
    "    if words in wikinews_model:\n",
    "      embedding_matrix[i] = wikinews_model[words]\n",
    "    else:\n",
    "      embedding_matrix[i] = wikinews_model['UNK']\n",
    "  \n",
    "  # embedding_matrix[max_features] = wikinews_model['UNK']\n",
    "  embedding_matrix[0] = np.array([0.0]*embed_size)\n",
    "  del wikinews_model\n",
    "  return embedding_matrix\n",
    "\n",
    "def load_glove2(word_index):\n",
    "  EMBEDDING_FILE = '/content/gdrive/My Drive/final_pj/embeddings/glove.840B.300d.txt'\n",
    "  def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "  embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "  all_embs = np.stack(embeddings_index.values())\n",
    "  emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "  embed_size = all_embs.shape[1]\n",
    "\n",
    "  # word_index = tokenizer.word_index\n",
    "  nb_words = min(max_features, len(word_index))\n",
    "  embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "  for word, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "      continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "      embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "      embedding_matrix[i] = embeddings_index.get('UNK')\n",
    "      \n",
    "  # embedding_matrix[max_features] = embeddings_index.get('UNK')\n",
    "  embedding_matrix[0] = np.array([0.0]*embed_size)\n",
    "  del embeddings_index\n",
    "  return embedding_matrix\n",
    "\n",
    "def load_googlenews(word_index): \n",
    "  GoogleNews = '/content/gdrive/My Drive/final_pj/embeddings/GoogleNews-vectors-negative300.bin.gz'\n",
    "  googlenews_model = gensim.models.KeyedVectors.load_word2vec_format(GoogleNews, binary=True)\n",
    "  \n",
    "  nb_words = min(max_features, len(word_index))\n",
    "  embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "  print(nb_words)\n",
    "  for words, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "      continue\n",
    "    if words in googlenews_model:\n",
    "      embedding_matrix[i] = googlenews_model[words]\n",
    "    else:\n",
    "      embedding_matrix[i] = googlenews_model['UNK']\n",
    "  \n",
    "  # embedding_matrix[max_features] = googlenews_model['UNK']\n",
    "  embedding_matrix[0] = np.array([0.0]*embed_size)\n",
    "  del googlenews_model\n",
    "  return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk5Ld23rKNzq"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "    \n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtF9UTYwKTsO"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n",
    "    for e in range(epochs):\n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks = callback, verbose=0)\n",
    "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "\n",
    "        best_score = metrics.f1_score(val_y, (pred_val_y > 0.33).astype(int))\n",
    "        print(\"Epoch: \", e, \"-    Val F1 Score: {:.4f}\".format(best_score))\n",
    "\n",
    "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "    print('=' * 60)\n",
    "    return pred_val_y, pred_test_y, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJvR3uQbrnwc"
   },
   "outputs": [],
   "source": [
    "# new implementation\n",
    "infile = open(\"/content/gdrive/My Drive/final_pj/embeddings/stopwords.txt\")\n",
    "stopwords = [line[:-1] for line in infile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 198853,
     "status": "ok",
     "timestamp": 1554724354661,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "UTYp6QqylIfw",
    "outputId": "b0362dc9-fc46-4d23-f269-28ace7fd0d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "# new implementation\n",
    "train_X, test_X, train_y, word_index = load_and_prec2(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 392033,
     "status": "ok",
     "timestamp": 1554724558534,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "AaK3rBIA9kZn",
    "outputId": "090b4669-d782-4292-b0b7-ba3d30cf1f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = load_glove2(word_index)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 323652,
     "status": "ok",
     "timestamp": 1554710914900,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "awdsFmHe9vOK",
    "outputId": "2cc93799-fbd7-4059-ab21-ccb4a0232582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# embedding_matrix = load_googlenews(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1DGA3MLKabD"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ryanzhang/tfidf-naivebayes-logreg-baseline\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlLfJ2Xrb3FS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,CuDNNLSTM,LSTM,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ock5WD0bKR0L"
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten(embedding_matrix_1):\n",
    "    \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix_1], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n",
    "    \n",
    "    # atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    # atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    # conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    \n",
    "    # conc = Dense(16, activation=\"relu\")(conc)\n",
    "    y = Flatten()(y)\n",
    "    conc = Dense(16, activation=\"relu\")(y)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1475,
     "status": "error",
     "timestamp": 1554697025246,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "bQvdUMIVMEQW",
    "outputId": "f3ee56e4-6d32-4bcc-ca18-5991161f5ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 70, 300)           15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 70, 80)            109440    \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 70, 80)            29280     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                89616     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,228,353\n",
      "Trainable params: 228,353\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.191878). Check your callbacks.\n",
      "Epoch:  0 -    Val F1 Score: 0.6447\n",
      "Epoch:  1 -    Val F1 Score: 0.6550\n",
      "Epoch:  2 -    Val F1 Score: 0.6574\n",
      "Epoch:  3 -    Val F1 Score: 0.6604\n",
      "Epoch:  4 -    Val F1 Score: 0.6588\n"
     ]
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2018\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "train_meta = np.zeros(train_y.shape)\n",
    "test_meta = np.zeros(test_X.shape[0])\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        X_train = train_X[train_idx]\n",
    "        y_train = train_y[train_idx]\n",
    "        X_val = train_X[valid_idx]\n",
    "        y_val = train_y[valid_idx]\n",
    "        model = model_lstm_atten(embedding_matrix)\n",
    "        pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = 8, callback = [clr,])\n",
    "        train_meta[valid_idx] = pred_val_y.reshape(-1)\n",
    "        test_meta += pred_test_y.reshape(-1) / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_452HIiWdjjj"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/content/gdrive/My Drive/Final Project/4995 DL/basic classification/sample_submission.csv')\n",
    "sub.prediction = test_meta > 0.33\n",
    "sub.to_csv(\"/content/gdrive/My Drive/Final Project/4995 DL/basic classification/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting unknown symbols from the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26790,
     "status": "ok",
     "timestamp": 1554601959302,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "ERnxhB4oRFNE",
    "outputId": "c99f8a80-3c47-48bc-e2c1-83de6b50a74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'did', 'Quebec', 'nationalists', 'see', 'their', 'province', 'as', 'a', 'nation', 'in', 'the', '1960s']\n"
     ]
    }
   ],
   "source": [
    "from keras import preprocessing\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "# count the number of unknown words\n",
    "train_df = pd.read_csv(\"/content/gdrive/My Drive/final_pj/train.csv\")\n",
    "    \n",
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"]\n",
    "train_X = [preprocessing.text.text_to_word_sequence(text, lower=False) for text in train_X]\n",
    "print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 141877,
     "status": "ok",
     "timestamp": 1554587797696,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "Q4ZBtKC5TpQe",
    "outputId": "dac3735f-ad4e-451f-a6cd-c094762d5c81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "GoogleNews = '/content/gdrive/My Drive/final_pj/embeddings/GoogleNews-vectors-negative300.bin.gz'\n",
    "googlenews_model = gensim.models.KeyedVectors.load_word2vec_format(GoogleNews, binary=True)\n",
    "word_list = googlenews_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10078,
     "status": "ok",
     "timestamp": 1554588432760,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "3i7CtDZ5f-ZX",
    "outputId": "99cb2166-e54a-4ec4-f2b6-4d734bd6b48c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sentence: 1306122\n",
      "sentence with UNK: 354582\n",
      "total_words: 16856728\n",
      "words with UNK: 474521\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "total_sent = 0\n",
    "unk_sent = 0\n",
    "unk_words = 0\n",
    "for line in train_X:\n",
    "  total_sent += 1\n",
    "  \n",
    "  flag = False\n",
    "  for w in line:\n",
    "    total_words += 1\n",
    "    if w not in word_list:\n",
    "      if not is_number(w) and w not in stopwords:\n",
    "        unk_words += 1\n",
    "        if flag == False:\n",
    "          flag = True\n",
    "          unk_sent += 1\n",
    "print('total_sentence:', total_sent)\n",
    "print('sentence with UNK:', unk_sent)\n",
    "print('total_words:', total_words)\n",
    "print('words with UNK:', unk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 281650,
     "status": "ok",
     "timestamp": 1554604427437,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "kz5g3olrsZtt",
    "outputId": "cc46e2f8-71ef-4daf-acc3-518b7af4a7f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "WIKI_NEWS = '/content/gdrive/My Drive/final_pj/embeddings/wiki-news-300d-1M.vec'\n",
    "wikinews_model = gensim.models.KeyedVectors.load_word2vec_format(WIKI_NEWS, binary=False)\n",
    "word_list = wikinews_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8309,
     "status": "ok",
     "timestamp": 1554589369533,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "ykKjUWmiuEEq",
    "outputId": "836e1c86-cfc5-45b9-d8ca-54c69b090a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sentence: 1306122\n",
      "sentence with UNK: 319679\n",
      "total_words: 16856728\n",
      "words with UNK: 409323\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "total_sent = 0\n",
    "unk_sent = 0\n",
    "unk_words = 0\n",
    "for line in train_X:\n",
    "  total_sent += 1\n",
    "  \n",
    "  flag = False\n",
    "  for w in line:\n",
    "    total_words += 1\n",
    "    if w not in word_list:\n",
    "      if not is_number(w) and w not in stopwords:\n",
    "        unk_words += 1\n",
    "        if flag == False:\n",
    "          flag = True\n",
    "          unk_sent += 1\n",
    "print('total_sentence:', total_sent)\n",
    "print('sentence with UNK:', unk_sent)\n",
    "print('total_words:', total_words)\n",
    "print('words with UNK:', unk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 95870,
     "status": "ok",
     "timestamp": 1554604556117,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "DkRcoUwU_Kff",
    "outputId": "407abdb5-28e2-4f37-ed06-597394f69899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "GLOVE_NEWS = '/content/gdrive/My Drive/final_pj/embeddings/glove.840B.300d.txt'\n",
    "glove_news = open(GLOVE_NEWS, 'rb')\n",
    "word_set = set()\n",
    "for line in glove_news:\n",
    "  line = line.decode(encoding=\"utf8\", errors='ignore').rstrip('\\n')\n",
    "  word_set.add(line.split()[0])\n",
    "glove_news.close()\n",
    "print('NUM' in word_set)\n",
    "print('UNK' in word_set)\n",
    "print('EOS' in word_set)\n",
    "print('' in word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7469,
     "status": "ok",
     "timestamp": 1554604607361,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "myJquAhQ_Kp9",
    "outputId": "169617b6-a13c-4849-b3ad-8a2d9ca57c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sentence: 1306122\n",
      "sentence with UNK: 173013\n",
      "total_words: 16856728\n",
      "words with UNK: 208634\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "total_sent = 0\n",
    "unk_sent = 0\n",
    "unk_words = 0\n",
    "for line in train_X:\n",
    "  total_sent += 1\n",
    "  \n",
    "  flag = False\n",
    "  for w in line:\n",
    "    total_words += 1\n",
    "    if w not in word_set:\n",
    "      if not is_number(w) and w not in stopwords:\n",
    "        unk_words += 1\n",
    "        if flag == False:\n",
    "          flag = True\n",
    "          unk_sent += 1\n",
    "print('total_sentence:', total_sent)\n",
    "print('sentence with UNK:', unk_sent)\n",
    "print('total_words:', total_words)\n",
    "print('words with UNK:', unk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42406,
     "status": "ok",
     "timestamp": 1554604819772,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "26Ev9wqFXcrc",
    "outputId": "6d19059c-e0c6-4224-c2e6-240a952a2ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "PARA_NEWS = '/content/gdrive/My Drive/final_pj/embeddings/paragram_300_sl999.txt'\n",
    "para_news = open(PARA_NEWS, 'rb')\n",
    "word_set = set()\n",
    "for line in para_news:\n",
    "  line = line.decode(encoding=\"utf8\", errors='ignore').rstrip('\\n')\n",
    "  word_set.add(line.split()[0])\n",
    "\n",
    "para_news.close()\n",
    "\n",
    "print('NUM' in word_set)\n",
    "print('UNK' in word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25940,
     "status": "ok",
     "timestamp": 1554604852078,
     "user": {
      "displayName": "Zian Zhao",
      "photoUrl": "",
      "userId": "04829576475632571911"
     },
     "user_tz": 240
    },
    "id": "K4VmNHp4XfK5",
    "outputId": "c2472f35-3b43-4420-ae2f-42ea8df7d58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sentence: 1306122\n",
      "sentence with UNK: 1302879\n",
      "total_words: 16856728\n",
      "words with UNK: 2925797\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "total_sent = 0\n",
    "unk_sent = 0\n",
    "unk_words = 0\n",
    "for line in train_X:\n",
    "  total_sent += 1\n",
    "  \n",
    "  flag = False\n",
    "  for w in line:\n",
    "    total_words += 1\n",
    "    if w not in word_set:\n",
    "      if not is_number(w) and w not in stopwords:\n",
    "        unk_words += 1\n",
    "        if flag == False:\n",
    "          flag = True\n",
    "          unk_sent += 1\n",
    "print('total_sentence:', total_sent)\n",
    "print('sentence with UNK:', unk_sent)\n",
    "print('total_words:', total_words)\n",
    "print('words with UNK:', unk_words)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Quora Insincere Questions Classification basic classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
